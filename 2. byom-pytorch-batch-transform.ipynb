{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e273dbd8",
   "metadata": {},
   "source": [
    "# Batch Transform 사용하기\n",
    "- 기본적으로 sagemaker endpoint를 띄울 때, 사용했던 inference.py를 동일하게 활용함.\n",
    "    - 그러므로, 하나의 코드로 endpoint와 batch transform 두 가지 태스크를 하려면, 분기를 만드는게 좋은 듯."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e75ec",
   "metadata": {},
   "source": [
    "## 1. Batch Transform에 대한 간단한 이해\n",
    "- 기본적으로 byom의 endpoint와 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8a55e",
   "metadata": {},
   "source": [
    "## 2. Batch Transform with SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3df425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "instance_type = \"ml.g4dn.xlarge\"\n",
    "model_artifact_path = \"s3://kdw-sagemaker/model/pytorch3/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80dcb91",
   "metadata": {},
   "source": [
    "- PyTorchModel SageMaker SDK 정의하기\n",
    "    - endpoint 띄우는 것과 동일함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1924383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(\n",
    "    entry_point=\"inference.py\", # inference.py의 파일명.\n",
    "    role=role, # role\n",
    "    model_data=model_artifact_path, # model_artifact의 경로\n",
    "    framework_version=\"1.8.1\", # pytorch version\n",
    "    py_version=\"py3\" # python version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13a2f7",
   "metadata": {},
   "source": [
    "- transformer를 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4e9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = model.transformer(1, instance_type, output_path=\"s3://kdw-sagemaker/data\", strategy=\"MultiRecord\", assemble_with=\"Line\", accept = \"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66210e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_location = \"s3://kdw-sagemaker/data/1000_row.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594af51d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................\u001b[34mCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (4.59.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.3.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mCollecting zipp>=0.5\n",
      "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: zipp, importlib-metadata, regex, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed click-8.0.1 filelock-3.0.12 huggingface-hub-0.0.8 importlib-metadata-4.3.0 regex-2021.4.4 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1 zipp-3.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:15,932 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.3.1\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 2756 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 1\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:15,962 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,692 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag cd448d2c4c6e4344948c031477a0bc3a\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,701 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,710 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,818 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,819 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,823 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,848 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,848 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]49\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,848 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,852 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,692 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag cd448d2c4c6e4344948c031477a0bc3a\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,701 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,710 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,818 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,819 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,823 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,848 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,848 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]49\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,848 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,852 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,857 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,879 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,967 [INFO ] pool-1-thread-2 ACCESS_LOG - /169.254.255.130:49720 \"GET /ping HTTP/1.1\" 200 19\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:22,968 [INFO ] pool-1-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,026 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:49724 \"GET /execution-parameters HTTP/1.1\" 404 6\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,030 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,269 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,270 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:89.88870239257812|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,270 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.736892700195312|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,271 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:17.2|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,271 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14297.7421875|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,272 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1140.890625|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,272 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.2|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,857 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,879 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,967 [INFO ] pool-1-thread-2 ACCESS_LOG - /169.254.255.130:49720 \"GET /ping HTTP/1.1\" 200 19\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:22,968 [INFO ] pool-1-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,026 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:49724 \"GET /execution-parameters HTTP/1.1\" 404 6\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,030 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,269 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,270 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:89.88870239257812|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,270 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.736892700195312|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,271 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:17.2|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,271 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14297.7421875|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,272 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1140.890625|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,272 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.2|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208683\u001b[0m\n",
      "\u001b[32m2021-05-28T13:31:23.080:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:23,995 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:23,995 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:27,562 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4626\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:27,563 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:4857|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208687\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:27,563 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:52|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:27,562 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4626\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:27,563 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:4857|#Level:Host|#hostname:ffacb72b521d,timestamp:1622208687\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:27,563 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:52|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\n",
      "\u001b[34m2021-05-28 13:31:39,325 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:39,326 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:4243|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-05-28 13:31:39,326 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:39,325 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:39,326 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:4243|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 13:31:39,326 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:ffacb72b521d,timestamp:null\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    input_location, \n",
    "    split_type=\"Line\",\n",
    "    content_type=\"application/json\",\n",
    "    job_name=\"kdw-batch-test-1\",\n",
    "    input_filter = \"$.text\",\n",
    "    join_source=\"Input\",\n",
    "    output_filter=\"$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ded96b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a991e",
   "metadata": {},
   "source": [
    "- 성공했던 job을 불러와서 똑같은 로직으로 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d680f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95c1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ebe15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime= boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20214971",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft= Transformer.attach(transform_job_name='kdw-batch-test-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e7b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_location = \"s3://kdw-sagemaker/data/1000_row2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed048a8b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................\u001b[34mCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.3.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (4.59.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mCollecting zipp>=0.5\n",
      "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: zipp, importlib-metadata, regex, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed click-8.0.1 filelock-3.0.12 huggingface-hub-0.0.8 importlib-metadata-4.3.0 regex-2021.4.4 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1 zipp-3.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:45,528 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.3.1\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 2756 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 1\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:45,559 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,279 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 743a40489c1143e8a480ffbcdcb38006\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,288 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,298 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,393 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,393 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,403 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,421 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,421 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]49\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,422 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,423 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,429 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,448 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,554 [INFO ] pool-1-thread-2 ACCESS_LOG - /169.254.255.130:54816 \"GET /ping HTTP/1.1\" 200 14\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,564 [INFO ] pool-1-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:cbdcf7434823,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,688 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:54822 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,690 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:cbdcf7434823,timestamp:null\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,834 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:cbdcf7434823,timestamp:1622209792\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,835 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:89.88868713378906|#Level:Host|#hostname:cbdcf7434823,timestamp:1622209792\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,835 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.736907958984375|#Level:Host|#hostname:cbdcf7434823,timestamp:1622209792\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,836 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:17.2|#Level:Host|#hostname:cbdcf7434823,timestamp:1622209792\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,836 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14343.3125|#Level:Host|#hostname:cbdcf7434823,timestamp:1622209792\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,837 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1094.85546875|#Level:Host|#hostname:cbdcf7434823,timestamp:1622209792\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:52,837 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:8.9|#Level:Host|#hostname:cbdcf7434823,timestamp:1622209792\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:53,626 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:57,270 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4745\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:57,270 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:4977|#Level:Host|#hostname:cbdcf7434823,timestamp:1622209797\u001b[0m\n",
      "\u001b[34m2021-05-28 13:49:57,270 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:57|#Level:Host|#hostname:cbdcf7434823,timestamp:null\u001b[0m\n",
      "\u001b[32m2021-05-28T13:49:52.725:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "\u001b[34m2021-05-28 13:50:09,402 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12130\u001b[0m\n",
      "\u001b[34m2021-05-28 13:50:09,402 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:12126.54|#ModelName:model,Level:Model|#hostname:cbdcf7434823,requestID:b522fadd-6887-4079-aff9-7f68c6b91bac,timestamp:1622209809\u001b[0m\n",
      "\u001b[34m2021-05-28 13:50:09,403 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:54826 \"POST /invocations HTTP/1.1\" 200 16479\u001b[0m\n",
      "\u001b[34m2021-05-28 13:50:09,403 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:cbdcf7434823,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-05-28 13:50:09,404 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:4338|#Level:Host|#hostname:cbdcf7434823,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-05-28 13:50:09,404 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:cbdcf7434823,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 13:50:09,402 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12130\u001b[0m\n",
      "\u001b[35m2021-05-28 13:50:09,402 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:12126.54|#ModelName:model,Level:Model|#hostname:cbdcf7434823,requestID:b522fadd-6887-4079-aff9-7f68c6b91bac,timestamp:1622209809\u001b[0m\n",
      "\u001b[35m2021-05-28 13:50:09,403 [INFO ] W-9000-model_1 ACCESS_LOG - /169.254.255.130:54826 \"POST /invocations HTTP/1.1\" 200 16479\u001b[0m\n",
      "\u001b[35m2021-05-28 13:50:09,403 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:cbdcf7434823,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 13:50:09,404 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:4338|#Level:Host|#hostname:cbdcf7434823,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 13:50:09,404 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:cbdcf7434823,timestamp:null\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tft.transform(\n",
    "    input_location, \n",
    "    split_type=\"Line\",\n",
    "    content_type=\"application/json\",\n",
    "    job_name=\"kdw-batch-test-2\",\n",
    "    input_filter = \"$.text\",\n",
    "    join_source=\"Input\",\n",
    "    output_filter=\"$\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e221d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdw-batch-test-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "1000_row2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe0f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3836208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime= boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime.invoke_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da5cdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Transformer.transform of <sagemaker.transformer.Transformer object at 0x7fccfbc5dcc0>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73736a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................\u001b[34mCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (4.59.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata\n",
      "  Downloading importlib_metadata-4.3.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting zipp>=0.5\n",
      "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting click\n",
      "  Downloading click-8.0.1-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: zipp, importlib-metadata, regex, filelock, click, tokenizers, sacremoses, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed click-8.0.1 filelock-3.0.12 huggingface-hub-0.0.8 importlib-metadata-4.3.0 regex-2021.4.4 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1 zipp-3.4.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:27,764 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.3.1\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 2748 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.6\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model.mar\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 1\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:27,793 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,490 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag bbee883546bc4c85a9bda6e6ea636ff9\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,502 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,517 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,490 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag bbee883546bc4c85a9bda6e6ea636ff9\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,502 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,517 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,628 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,628 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,629 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,658 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,658 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]50\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,658 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,659 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,664 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,681 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,780 [INFO ] pool-1-thread-2 ACCESS_LOG - /169.254.255.130:47704 \"GET /ping HTTP/1.1\" 200 23\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,780 [INFO ] pool-1-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,850 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47708 \"GET /execution-parameters HTTP/1.1\" 404 4\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:34,852 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:35,063 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:35,064 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:89.88867568969727|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:35,064 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.736919403076172|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:35,064 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:17.2|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:35,065 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14326.27734375|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:35,065 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1108.2265625|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,628 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,628 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,629 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,658 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,658 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]50\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,658 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,659 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,664 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,681 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,780 [INFO ] pool-1-thread-2 ACCESS_LOG - /169.254.255.130:47704 \"GET /ping HTTP/1.1\" 200 23\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,780 [INFO ] pool-1-thread-2 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,850 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47708 \"GET /execution-parameters HTTP/1.1\" 404 4\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:34,852 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:35,063 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:35,064 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:89.88867568969727|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:35,064 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.736919403076172|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:35,064 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:17.2|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:35,065 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14326.27734375|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:35,065 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1108.2265625|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:35,065 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.0|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:35,065 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:9.0|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205635\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:35,894 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:35,894 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[32m2021-05-28T12:40:34.907:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:39,474 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4711\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:39,475 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:4966|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205639\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:39,475 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:66|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:39,474 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4711\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:39,475 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:4966|#Level:Host|#hostname:a88acec0d6f7,timestamp:1622205639\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:39,475 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:66|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\n",
      "\u001b[32m2021-05-28T12:40:51.482:[sagemaker logs]: kdw-sagemaker/data/1000_row.json: ClientError: 400\u001b[0m\n",
      "\u001b[32m2021-05-28T12:40:51.482:[sagemaker logs]: kdw-sagemaker/data/1000_row.json: \u001b[0m\n",
      "\u001b[32m2021-05-28T12:40:51.482:[sagemaker logs]: kdw-sagemaker/data/1000_row.json: Message:\u001b[0m\n",
      "\u001b[32m2021-05-28T12:40:51.482:[sagemaker logs]: kdw-sagemaker/data/1000_row.json: Failed to unmarshal algorithm data: '[\u001b[0m\n",
      "\u001b[32m2021-05-28T12:40:51.482:[sagemaker logs]: kdw-sagemaker/data/1000_row.json: ' to JSON: unexpected end of JSON input\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:51,465 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:51,465 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:4346|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[34m2021-05-28 12:40:51,466 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:51,465 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:51,465 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:4346|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n",
      "\u001b[35m2021-05-28 12:40:51,466 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:a88acec0d6f7,timestamp:null\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job test-batch-5: Failed. Reason: ClientError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8c8674c26f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minput_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"$.text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mjoin_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Input\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutput_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"$['id','SageMakerOutput']\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, wait, logs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_transform_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/transformer.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3851\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TransformJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3852\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3853\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3243\u001b[0m                 ),\n\u001b[1;32m   3244\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3245\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3246\u001b[0m             )\n\u001b[1;32m   3247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job test-batch-5: Failed. Reason: ClientError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    input_location, \n",
    "    split_type=\"Line\",\n",
    "    content_type=\"application/json\",\n",
    "    job_name=\"test-batch-6\",\n",
    "    input_filter = \"$.text\",\n",
    "    join_source=\"Input\",\n",
    "    output_filter=\"$['id','SageMakerOutput']\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eacdd4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6275a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5699aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57610199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance({'sl':'sl'}, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "733829fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.61.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: tqdm, filelock, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.0.12 huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 tqdm-4.61.0 transformers-4.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d26b985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87a1e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f32062f4a6f46b59df4d9f546fdd63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab2ee89dc0543efb3de673735e370a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/250k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4baf15ab18c4d2c93f17c7a1f051269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13fbdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data =['시발','개발','야발']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61372291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 13552,     3]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "{'input_ids': tensor([[   2, 9981,    3]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "{'input_ids': tensor([[   2, 2207, 4235,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "for each in input_data:\n",
    "    result= tokenizer(each, return_tensors='pt')\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80886d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "model_dir = '../model'\n",
    "config = AutoConfig.from_pretrained(os.path.join(model_dir, 'config.json'))\n",
    "model = AutoModelForTokenClassification.from_pretrained(os.path.join(model_dir, 'pytorch_model.bin'), config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7416be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "id2label = np.array(list(map(lambda x:x[1], sorted(model.config.id2label.items(), key=lambda x: int(x[0])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24549939",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = id2label[model(**tokenizer('아모레퍼시픽 배고파', return_tensors='pt')).logits.cpu().argmax(axis=-1).numpy()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16a31bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=[]\n",
    "tmp += result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ad0a6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'ORG-B', 'ORG-B', 'ORG-B', 'O', 'O', 'O', 'O', 'O', 'O']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bdbf2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp += result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7efd95a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'ORG-B', 'ORG-B', 'ORG-B', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'ORG-B', 'ORG-B', 'ORG-B', 'O', 'O', 'O', 'O', 'O', 'O']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d06635e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'PER-B',\n",
       " 10: 'LOC-I',\n",
       " 11: 'CVL-B',\n",
       " 12: 'CVL-I',\n",
       " 13: 'DAT-B',\n",
       " 14: 'DAT-I',\n",
       " 15: 'TIM-B',\n",
       " 16: 'TIM-I',\n",
       " 17: 'NUM-B',\n",
       " 18: 'NUM-I',\n",
       " 19: 'EVT-B',\n",
       " 2: 'PER-I',\n",
       " 20: 'EVT-I',\n",
       " 21: 'ANM-B',\n",
       " 22: 'ANM-I',\n",
       " 23: 'PLT-B',\n",
       " 24: 'PLT-I',\n",
       " 25: 'MAT-B',\n",
       " 26: 'MAT-I',\n",
       " 27: 'TRM-B',\n",
       " 28: 'TRM-I',\n",
       " 3: 'FLD-B',\n",
       " 4: 'FLD-I',\n",
       " 5: 'AFW-B',\n",
       " 6: 'AFW-I',\n",
       " 7: 'ORG-B',\n",
       " 8: 'ORG-I',\n",
       " 9: 'LOC-B'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2b4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0760766",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 4)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2724197c2a35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2303\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2305\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2306\u001b[0m             )\n\u001b[1;32m   2307\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2490\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2491\u001b[0m         )\n\u001b[1;32m   2492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanitized_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eventual_warn_about_too_long_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     def _encode_plus(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    714\u001b[0m                     )\n\u001b[1;32m    715\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 716\u001b[0;31m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m                     \u001b[0;34m\"with 'padding=True' 'truncation=True' to have batched tensors with the same length.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "tokenizer(each, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c85bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f14d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af46c33",
   "metadata": {},
   "outputs": [],
   "source": [
    ",    input_filter=\"$[1:]\",\n",
    "    join_source=\"Input\",\n",
    "    output_filter=\"$[1,-1]\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bfb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Args:\n",
    "    instance_count (int): Number of EC2 instances to use.\n",
    "    instance_type (str): Type of EC2 instance to use, for example,\n",
    "        'ml.c4.xlarge'.\n",
    "    strategy (str): The strategy used to decide how to batch records in\n",
    "        a single request (default: None). Valid values: 'MultiRecord'\n",
    "        and 'SingleRecord'.\n",
    "    assemble_with (str): How the output is assembled (default: None).\n",
    "        Valid values: 'Line' or 'None'.\n",
    "    output_path (str): S3 location for saving the transform result. If\n",
    "        not specified, results are stored to a default bucket.\n",
    "    output_kms_key (str): Optional. KMS key ID for encrypting the\n",
    "        transform output (default: None).\n",
    "    accept (str): The accept header passed by the client to\n",
    "        the inference endpoint. If it is supported by the endpoint,\n",
    "        it will be the format of the batch transform output.\n",
    "    env (dict): Environment variables to be set for use during the\n",
    "        transform job (default: None).\n",
    "    max_concurrent_transforms (int): The maximum number of HTTP requests\n",
    "        to be made to each individual transform container at one time.\n",
    "    max_payload (int): Maximum size of the payload in a single HTTP\n",
    "        request to the container in MB.\n",
    "    tags (list[dict]): List of tags for labeling a transform job. If\n",
    "        none specified, then the tags used for the training job are used\n",
    "        for the transform job.\n",
    "    volume_kms_key (str): Optional. KMS key ID for encrypting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f407c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transformer(\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cabb144",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.transformer.Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822ca2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b51de44",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e051398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac0b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('1000_row.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81089fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>483</td>\n",
       "      <td>348869-173514688259</td>\n",
       "      <td>기장 아난티 코브 라메르 점심 코스 후기 안녕하세요 숨니입니다 얼마 전 가족 이벤트...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>841</td>\n",
       "      <td>365863-173514187718</td>\n",
       "      <td>* 제 품 협 찬 *오전에는 10도, 오후에는 25도.현타가 제대로 오는 일교차의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>842</td>\n",
       "      <td>365863-173497341171</td>\n",
       "      <td>언니 오늘 영상두 잘봤어요!ㅎㅎ 너무 유익했어요ㅠㅜㅜㅜ요즘 고데기 진짜 많이 하는데...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>843</td>\n",
       "      <td>365863-173513479577</td>\n",
       "      <td>@된장님 원장찌개 배달왔어요 ㅇㅈㅋㅋㅋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>859</td>\n",
       "      <td>365863-173513480317</td>\n",
       "      <td>근데 확실히 연기에 집중하던 배우들이랑 CF에 집중하던 배우들이랑은 평가나 수명이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>843</td>\n",
       "      <td>346527-173535865239</td>\n",
       "      <td>아니 뭐야 나빼고 다 1일 전이에요? 나만 늦게 온 거야?? 이런...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>850</td>\n",
       "      <td>346527-173515523339</td>\n",
       "      <td>네오쿠션 17N 좀 핑크끼 돌아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>888</td>\n",
       "      <td>346527-173515522025</td>\n",
       "      <td>그럼 둘다 밝기 정도는 비슷해..?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1046</td>\n",
       "      <td>346527-173538605731</td>\n",
       "      <td>유난히도 피부가 촉촉하고 피부 결이 예뻐보이는 날이 있다.그 때마다 내가 어떤 스킨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2028</td>\n",
       "      <td>346527-173535867770</td>\n",
       "      <td>@조조 그런걸 벗겨내고 연기하는게 배우죠 김하늘도 데뷔초기엔 우울함 밖에 없던 배우...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                   id  \\\n",
       "0           483  348869-173514688259   \n",
       "1           841  365863-173514187718   \n",
       "2           842  365863-173497341171   \n",
       "3           843  365863-173513479577   \n",
       "4           859  365863-173513480317   \n",
       "..          ...                  ...   \n",
       "995         843  346527-173535865239   \n",
       "996         850  346527-173515523339   \n",
       "997         888  346527-173515522025   \n",
       "998        1046  346527-173538605731   \n",
       "999        2028  346527-173535867770   \n",
       "\n",
       "                                                  text  \n",
       "0    기장 아난티 코브 라메르 점심 코스 후기 안녕하세요 숨니입니다 얼마 전 가족 이벤트...  \n",
       "1    * 제 품 협 찬 *오전에는 10도, 오후에는 25도.현타가 제대로 오는 일교차의 ...  \n",
       "2    언니 오늘 영상두 잘봤어요!ㅎㅎ 너무 유익했어요ㅠㅜㅜㅜ요즘 고데기 진짜 많이 하는데...  \n",
       "3                                @된장님 원장찌개 배달왔어요 ㅇㅈㅋㅋㅋ  \n",
       "4    근데 확실히 연기에 집중하던 배우들이랑 CF에 집중하던 배우들이랑은 평가나 수명이 ...  \n",
       "..                                                 ...  \n",
       "995            아니 뭐야 나빼고 다 1일 전이에요? 나만 늦게 온 거야?? 이런...  \n",
       "996                                  네오쿠션 17N 좀 핑크끼 돌아  \n",
       "997                                그럼 둘다 밝기 정도는 비슷해..?  \n",
       "998  유난히도 피부가 촉촉하고 피부 결이 예뻐보이는 날이 있다.그 때마다 내가 어떤 스킨...  \n",
       "999  @조조 그런걸 벗겨내고 연기하는게 배우죠 김하늘도 데뷔초기엔 우울함 밖에 없던 배우...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fe6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"s3://kdw-sagemaker/data/1000_row.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65784dd",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html\n",
    "- https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker_batch_transform/batch_transform_associate_predictions_with_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
