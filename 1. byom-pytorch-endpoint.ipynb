{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398100f3",
   "metadata": {},
   "source": [
    "# SageMaker SDK를 활용하여 서빙하기. (a.k.a Bring Your Own Model)\n",
    "<img src = \"deploy-sagemaker.jpg\">\n",
    "\n",
    "\n",
    "### 대상\n",
    "- 로컬에서 직접하는 모델을 sagemaker를 활용하여 서빙만 하고 싶은 경우.\n",
    "- api 서버 개발은 하지 않고 오직 모델만 서빙하고 싶은 경우."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a44ef",
   "metadata": {},
   "source": [
    "## 1. Model Artifact를 만들기.\n",
    "- 모델 추론을 위한 코드와 관련 Asset들을 포함한 압축파일.\n",
    "- 모델 artifact는 실제 deploy를 할 때, aws에서 제공하는 도커 환경에 decompressed되는 파일.\n",
    "    - **정해진 경로와 네이밍을 잘 맞추는 것이 중요함.**\n",
    "\n",
    "### 1.1. 모델 artifact의 구조\n",
    "```bash\n",
    "# tar file을 만들 때, 실제 model 경로 안에 들어가서 `tar -zcvf model.tar.gz .`을 실행해야 됨.\n",
    "\n",
    "model.tar.gz/\n",
    "|- model.bin # model binary.\n",
    "|- ... # assets.\n",
    "|- code/\n",
    "  |- inference.py # inference code,\n",
    "  |- requirements.txt  # python packages need to be installed.\n",
    "```\n",
    "\n",
    "**필수**\n",
    "- model.bin (모델 바이너리 파일)\n",
    "- inference.py (모델 추론 관련 코드)\n",
    "\n",
    "**optional**\n",
    "- config.json\n",
    "- vocab.txt (토크나이저)\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb4df5",
   "metadata": {},
   "source": [
    "### 1.2. `inference.py`를 정의하기.\n",
    "- `inference.py`은 아래의 네 가지 function 정의해야 됨.\n",
    "    - `model_fn`\n",
    "        - 추론에 사용할 모델을 로드하는 부분.\n",
    "        - return을 tuple 또는 dictionary로 받으면 여러개의 return을 받을 수 있다.\n",
    "    - `input_fn`: request data를 deserialize하고, prediction을 위한 input 형태로 변환하는 부분.\n",
    "        - 다양한 request_content_type에 따라서 request body가 어떻게 들어오는지 확인 필요함.\n",
    "    - `predict_fn`: 실제 inference 코드를 수행하는 부분.\n",
    "    - `output_fn`: result of prediction을 serialize하여, 리턴하는 부분\n",
    "    - 함수의 input과 output은 다음과 같은 flow로 흘러감.\n",
    "        - (model_fn, input_fn) $\\longrightarrow$ predict_fn $\\longrightarrow$ output_fn\n",
    "- input, ouput의 가장 간단한 방식은 json으로 데이터를 주고 받는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed956e02",
   "metadata": {},
   "source": [
    "## 2. Deploy Model with SageMaker SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80765d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287f9e9",
   "metadata": {},
   "source": [
    "- role과 필요한 변수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc2d7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "model_artifact_path = \"s3://kdw-sagemaker/model/pytorch2/model.tar.gz\"\n",
    "instance_type = \"ml.g4dn.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542d4e8",
   "metadata": {},
   "source": [
    "- PyTorchModel SageMaker SDK 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cf3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(\n",
    "    entry_point=\"inference.py\", # inference.py의 파일명.\n",
    "    role=role, # role\n",
    "    model_data=model_artifact_path, # model_artifact의 경로\n",
    "    framework_version=\"1.8.1\", # pytorch version\n",
    "    py_version=\"py3\" # python version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78e833",
   "metadata": {},
   "source": [
    "- model deploy하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8e92b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    endpoint_name=\"naver-ner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32a510",
   "metadata": {},
   "source": [
    "## 3. Call the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ced02",
   "metadata": {},
   "source": [
    "### 3-1. SageMaker SDK 사용하기\n",
    "- model deploy가 된 객체가 있어야됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55a08a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: {'predicts': [['O', 'ORG-B', 'ORG-B', 'ORG-B', 'ORG-B', 'ORG-B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]}\n",
      "CPU times: user 0 ns, sys: 3.32 ms, total: 3.32 ms\n",
      "Wall time: 36.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dummy_data = {'text': '아모레퍼시픽은 화장품 회사다.'}\n",
    "res = predictor.predict(dummy_data)\n",
    "print(\"Predictions:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5ae15",
   "metadata": {},
   "source": [
    "### 3-2. Boto3로 call 하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89c4cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37bbfa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime= boto3.client('runtime.sagemaker')\n",
    "endpoint_name = \"naver-ner\"\n",
    "payload = json.dumps({'text': '아모레퍼시픽은 화장품 회사다.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "277203d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicts': [['O', 'ORG-B', 'ORG-B', 'ORG-B', 'ORG-B', 'ORG-B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]}\n",
      "CPU times: user 3.13 ms, sys: 0 ns, total: 3.13 ms\n",
      "Wall time: 22.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='application/json',\n",
    "                                       Body=payload)\n",
    "\n",
    "print(json.loads(response['Body'].read().decode()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
